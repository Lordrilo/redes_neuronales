{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "noe5nn1swu1a"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DEpz0Nwu1c"
      },
      "source": [
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the `torch.nn` package.\n",
        "\n",
        "Now that you had a glimpse of `autograd`, `nn` depends on `autograd` to\n",
        "define models and differentiate them. An `nn.Module` contains layers,\n",
        "and a method `forward(input)` that returns the `output`.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        "![convnet](https://pytorch.org/tutorials/_static/img/mnist.png)\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "-   Define the neural network that has some learnable parameters (or\n",
        "    weights)\n",
        "-   Iterate over a dataset of inputs\n",
        "-   Process input through the network\n",
        "-   Compute the loss (how far is the output from being correct)\n",
        "-   Propagate gradients back into the network's parameters\n",
        "-   Update the weights of the network, typically using a simple update\n",
        "    rule: `weight = weight - learning_rate * gradient`\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let's define this network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntB3yDL1wu1d",
        "outputId": "e211c18b-a363-4a68-db2c-0b2da7f2d3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 4*4 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    # operaciones\n",
        "    def forward(self, input):\n",
        "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a (N, 16, 10, 10) Tensor\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # Fully connected layer F5: (N, 400) Tensor input,\n",
        "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        # Fully connected layer F6: (N, 120) Tensor input,\n",
        "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
        "        # outputs a (N, 10) Tensor\n",
        "        output = self.fc3(f6)\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNiY7T9wu1e"
      },
      "source": [
        "You just have to define the `forward` function, and the `backward`\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using `autograd`. You can use any of the Tensor operations in the\n",
        "`forward` function.\n",
        "\n",
        "The learnable parameters of a model are returned by `net.parameters()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG3ESNOgwu1e",
        "outputId": "4ec210f9-194a-4b24-c52a-6e7dd34c497a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list(net.parameters()) # filtros con pesos aleatorios"
      ],
      "metadata": {
        "id": "1726reoTymZs"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar dataset"
      ],
      "metadata": {
        "id": "i4ucMeoYV5fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "# crear variables para entrenamiento para pruebas\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "print(training_data)\n",
        "print(test_data)\n",
        "clases=training_data.classes\n",
        "print(clases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypVIoh4b2OkE",
        "outputId": "0b0b67e9-d526-42d4-9b67-9356b783a781"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n",
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n",
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtener caractertisticas y etiquetas de las imágenes"
      ],
      "metadata": {
        "id": "OY1t9v-4Vh0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#obtener caracteristicas de imagenes y etiquetas\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Tamaño de batch, dimensiones y canales: {train_features.size()}\")\n",
        "print(f\"Número de etiquetas por cada batch: {train_labels.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eeIFmXk-3-g",
        "outputId": "b9230db9-1bdc-4a97-d963-b39d83fee1be"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de batch, dimensiones y canales: torch.Size([32, 1, 28, 28])\n",
            "Número de etiquetas por cada batch: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir función de perdida y optimizador"
      ],
      "metadata": {
        "id": "VfHAnvWfWoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterio = nn.CrossEntropyLoss()\n",
        "# momentum sirve para acelerar el proceso de encontrar el valor más cercano a cero\n",
        "optimizador = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "qBPyCIhzWvmb"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurar uso de GPU"
      ],
      "metadata": {
        "id": "3TNLdnv9Ngmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# elección de arquitectura\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Usando:\",device)\n",
        "\n",
        "# Mover el modelo a GPU/CPU\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2T0W9DwNmbF",
        "outputId": "cb945d07-0577-479d-993b-884a6a0da773"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo de CNN"
      ],
      "metadata": {
        "id": "gf7e25JeWQ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento de modelo\n",
        "mini_batch = 5000\n",
        "for epoca in range(20):  # número de épocas\n",
        "\n",
        "    running_loss = 0.0\n",
        "    umbral = mini_batch\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the entradas; data is a list of [entradas, etiquetas]\n",
        "        entradas, etiquetas = data\n",
        "        # mover datos a GPU/CPU\n",
        "        entradas, etiquetas = entradas.to(device), etiquetas.to(device)\n",
        "        # gradiente de ceros\n",
        "        optimizador.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        salidas = net(entradas) # envío\n",
        "        perdida = criterio(salidas, etiquetas) # calculo diferencia o pérdida\n",
        "        perdida.backward() # Actualiza pesos\n",
        "        optimizador.step() # optimizo\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += perdida.item() # perdida acumulada\n",
        "        if ((i+1) * batch_size) >= umbral:    # print every 5000 mini-batches\n",
        "            print(f'Época [{epoca + 1}, lote {i + 1:5d}] pérdida: {running_loss / (i+1):.3f}')\n",
        "            running_loss = 0.0\n",
        "            umbral += mini_batch\n",
        "\n",
        "print('Entrenamiento finalizado')"
      ],
      "metadata": {
        "id": "LhWRm5--WWzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f96b83-3327-42d7-dc0b-3ddc89bbc957"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [1, lote   157] pérdida: 1.767\n",
            "Época [1, lote   313] pérdida: 0.388\n",
            "Época [1, lote   469] pérdida: 0.224\n",
            "Época [1, lote   625] pérdida: 0.139\n",
            "Época [1, lote   782] pérdida: 0.108\n",
            "Época [1, lote   938] pérdida: 0.084\n",
            "Época [1, lote  1094] pérdida: 0.069\n",
            "Época [1, lote  1250] pérdida: 0.060\n",
            "Época [1, lote  1407] pérdida: 0.052\n",
            "Época [1, lote  1563] pérdida: 0.045\n",
            "Época [1, lote  1719] pérdida: 0.038\n",
            "Época [1, lote  1875] pérdida: 0.036\n",
            "Época [2, lote   157] pérdida: 0.400\n",
            "Época [2, lote   313] pérdida: 0.197\n",
            "Época [2, lote   469] pérdida: 0.128\n",
            "Época [2, lote   625] pérdida: 0.094\n",
            "Época [2, lote   782] pérdida: 0.076\n",
            "Época [2, lote   938] pérdida: 0.061\n",
            "Época [2, lote  1094] pérdida: 0.053\n",
            "Época [2, lote  1250] pérdida: 0.043\n",
            "Época [2, lote  1407] pérdida: 0.041\n",
            "Época [2, lote  1563] pérdida: 0.035\n",
            "Época [2, lote  1719] pérdida: 0.031\n",
            "Época [2, lote  1875] pérdida: 0.029\n",
            "Época [3, lote   157] pérdida: 0.323\n",
            "Época [3, lote   313] pérdida: 0.169\n",
            "Época [3, lote   469] pérdida: 0.108\n",
            "Época [3, lote   625] pérdida: 0.084\n",
            "Época [3, lote   782] pérdida: 0.066\n",
            "Época [3, lote   938] pérdida: 0.053\n",
            "Época [3, lote  1094] pérdida: 0.045\n",
            "Época [3, lote  1250] pérdida: 0.040\n",
            "Época [3, lote  1407] pérdida: 0.038\n",
            "Época [3, lote  1563] pérdida: 0.031\n",
            "Época [3, lote  1719] pérdida: 0.027\n",
            "Época [3, lote  1875] pérdida: 0.025\n",
            "Época [4, lote   157] pérdida: 0.302\n",
            "Época [4, lote   313] pérdida: 0.156\n",
            "Época [4, lote   469] pérdida: 0.101\n",
            "Época [4, lote   625] pérdida: 0.072\n",
            "Época [4, lote   782] pérdida: 0.058\n",
            "Época [4, lote   938] pérdida: 0.049\n",
            "Época [4, lote  1094] pérdida: 0.041\n",
            "Época [4, lote  1250] pérdida: 0.037\n",
            "Época [4, lote  1407] pérdida: 0.031\n",
            "Época [4, lote  1563] pérdida: 0.030\n",
            "Época [4, lote  1719] pérdida: 0.026\n",
            "Época [4, lote  1875] pérdida: 0.023\n",
            "Época [5, lote   157] pérdida: 0.274\n",
            "Época [5, lote   313] pérdida: 0.140\n",
            "Época [5, lote   469] pérdida: 0.092\n",
            "Época [5, lote   625] pérdida: 0.071\n",
            "Época [5, lote   782] pérdida: 0.056\n",
            "Época [5, lote   938] pérdida: 0.045\n",
            "Época [5, lote  1094] pérdida: 0.041\n",
            "Época [5, lote  1250] pérdida: 0.034\n",
            "Época [5, lote  1407] pérdida: 0.031\n",
            "Época [5, lote  1563] pérdida: 0.027\n",
            "Época [5, lote  1719] pérdida: 0.025\n",
            "Época [5, lote  1875] pérdida: 0.022\n",
            "Época [6, lote   157] pérdida: 0.254\n",
            "Época [6, lote   313] pérdida: 0.127\n",
            "Época [6, lote   469] pérdida: 0.090\n",
            "Época [6, lote   625] pérdida: 0.068\n",
            "Época [6, lote   782] pérdida: 0.056\n",
            "Época [6, lote   938] pérdida: 0.044\n",
            "Época [6, lote  1094] pérdida: 0.037\n",
            "Época [6, lote  1250] pérdida: 0.030\n",
            "Época [6, lote  1407] pérdida: 0.030\n",
            "Época [6, lote  1563] pérdida: 0.026\n",
            "Época [6, lote  1719] pérdida: 0.023\n",
            "Época [6, lote  1875] pérdida: 0.022\n",
            "Época [7, lote   157] pérdida: 0.234\n",
            "Época [7, lote   313] pérdida: 0.123\n",
            "Época [7, lote   469] pérdida: 0.082\n",
            "Época [7, lote   625] pérdida: 0.057\n",
            "Época [7, lote   782] pérdida: 0.054\n",
            "Época [7, lote   938] pérdida: 0.042\n",
            "Época [7, lote  1094] pérdida: 0.036\n",
            "Época [7, lote  1250] pérdida: 0.031\n",
            "Época [7, lote  1407] pérdida: 0.029\n",
            "Época [7, lote  1563] pérdida: 0.026\n",
            "Época [7, lote  1719] pérdida: 0.023\n",
            "Época [7, lote  1875] pérdida: 0.022\n",
            "Época [8, lote   157] pérdida: 0.237\n",
            "Época [8, lote   313] pérdida: 0.116\n",
            "Época [8, lote   469] pérdida: 0.080\n",
            "Época [8, lote   625] pérdida: 0.060\n",
            "Época [8, lote   782] pérdida: 0.048\n",
            "Época [8, lote   938] pérdida: 0.038\n",
            "Época [8, lote  1094] pérdida: 0.035\n",
            "Época [8, lote  1250] pérdida: 0.032\n",
            "Época [8, lote  1407] pérdida: 0.028\n",
            "Época [8, lote  1563] pérdida: 0.024\n",
            "Época [8, lote  1719] pérdida: 0.022\n",
            "Época [8, lote  1875] pérdida: 0.021\n",
            "Época [9, lote   157] pérdida: 0.210\n",
            "Época [9, lote   313] pérdida: 0.111\n",
            "Época [9, lote   469] pérdida: 0.077\n",
            "Época [9, lote   625] pérdida: 0.053\n",
            "Época [9, lote   782] pérdida: 0.047\n",
            "Época [9, lote   938] pérdida: 0.043\n",
            "Época [9, lote  1094] pérdida: 0.034\n",
            "Época [9, lote  1250] pérdida: 0.030\n",
            "Época [9, lote  1407] pérdida: 0.026\n",
            "Época [9, lote  1563] pérdida: 0.024\n",
            "Época [9, lote  1719] pérdida: 0.023\n",
            "Época [9, lote  1875] pérdida: 0.019\n",
            "Época [10, lote   157] pérdida: 0.221\n",
            "Época [10, lote   313] pérdida: 0.103\n",
            "Época [10, lote   469] pérdida: 0.075\n",
            "Época [10, lote   625] pérdida: 0.057\n",
            "Época [10, lote   782] pérdida: 0.044\n",
            "Época [10, lote   938] pérdida: 0.036\n",
            "Época [10, lote  1094] pérdida: 0.033\n",
            "Época [10, lote  1250] pérdida: 0.026\n",
            "Época [10, lote  1407] pérdida: 0.025\n",
            "Época [10, lote  1563] pérdida: 0.023\n",
            "Época [10, lote  1719] pérdida: 0.021\n",
            "Época [10, lote  1875] pérdida: 0.019\n",
            "Época [11, lote   157] pérdida: 0.222\n",
            "Época [11, lote   313] pérdida: 0.101\n",
            "Época [11, lote   469] pérdida: 0.071\n",
            "Época [11, lote   625] pérdida: 0.054\n",
            "Época [11, lote   782] pérdida: 0.043\n",
            "Época [11, lote   938] pérdida: 0.035\n",
            "Época [11, lote  1094] pérdida: 0.031\n",
            "Época [11, lote  1250] pérdida: 0.027\n",
            "Época [11, lote  1407] pérdida: 0.025\n",
            "Época [11, lote  1563] pérdida: 0.023\n",
            "Época [11, lote  1719] pérdida: 0.019\n",
            "Época [11, lote  1875] pérdida: 0.019\n",
            "Época [12, lote   157] pérdida: 0.195\n",
            "Época [12, lote   313] pérdida: 0.098\n",
            "Época [12, lote   469] pérdida: 0.068\n",
            "Época [12, lote   625] pérdida: 0.052\n",
            "Época [12, lote   782] pérdida: 0.043\n",
            "Época [12, lote   938] pérdida: 0.034\n",
            "Época [12, lote  1094] pérdida: 0.031\n",
            "Época [12, lote  1250] pérdida: 0.028\n",
            "Época [12, lote  1407] pérdida: 0.022\n",
            "Época [12, lote  1563] pérdida: 0.023\n",
            "Época [12, lote  1719] pérdida: 0.021\n",
            "Época [12, lote  1875] pérdida: 0.019\n",
            "Época [13, lote   157] pérdida: 0.182\n",
            "Época [13, lote   313] pérdida: 0.094\n",
            "Época [13, lote   469] pérdida: 0.070\n",
            "Época [13, lote   625] pérdida: 0.049\n",
            "Época [13, lote   782] pérdida: 0.038\n",
            "Época [13, lote   938] pérdida: 0.034\n",
            "Época [13, lote  1094] pérdida: 0.030\n",
            "Época [13, lote  1250] pérdida: 0.024\n",
            "Época [13, lote  1407] pérdida: 0.024\n",
            "Época [13, lote  1563] pérdida: 0.023\n",
            "Época [13, lote  1719] pérdida: 0.018\n",
            "Época [13, lote  1875] pérdida: 0.017\n",
            "Época [14, lote   157] pérdida: 0.195\n",
            "Época [14, lote   313] pérdida: 0.101\n",
            "Época [14, lote   469] pérdida: 0.064\n",
            "Época [14, lote   625] pérdida: 0.047\n",
            "Época [14, lote   782] pérdida: 0.038\n",
            "Época [14, lote   938] pérdida: 0.033\n",
            "Época [14, lote  1094] pérdida: 0.030\n",
            "Época [14, lote  1250] pérdida: 0.025\n",
            "Época [14, lote  1407] pérdida: 0.022\n",
            "Época [14, lote  1563] pérdida: 0.019\n",
            "Época [14, lote  1719] pérdida: 0.018\n",
            "Época [14, lote  1875] pérdida: 0.018\n",
            "Época [15, lote   157] pérdida: 0.171\n",
            "Época [15, lote   313] pérdida: 0.082\n",
            "Época [15, lote   469] pérdida: 0.065\n",
            "Época [15, lote   625] pérdida: 0.048\n",
            "Época [15, lote   782] pérdida: 0.040\n",
            "Época [15, lote   938] pérdida: 0.034\n",
            "Época [15, lote  1094] pérdida: 0.027\n",
            "Época [15, lote  1250] pérdida: 0.023\n",
            "Época [15, lote  1407] pérdida: 0.022\n",
            "Época [15, lote  1563] pérdida: 0.020\n",
            "Época [15, lote  1719] pérdida: 0.018\n",
            "Época [15, lote  1875] pérdida: 0.016\n",
            "Época [16, lote   157] pérdida: 0.165\n",
            "Época [16, lote   313] pérdida: 0.090\n",
            "Época [16, lote   469] pérdida: 0.057\n",
            "Época [16, lote   625] pérdida: 0.045\n",
            "Época [16, lote   782] pérdida: 0.036\n",
            "Época [16, lote   938] pérdida: 0.031\n",
            "Época [16, lote  1094] pérdida: 0.026\n",
            "Época [16, lote  1250] pérdida: 0.025\n",
            "Época [16, lote  1407] pérdida: 0.022\n",
            "Época [16, lote  1563] pérdida: 0.019\n",
            "Época [16, lote  1719] pérdida: 0.018\n",
            "Época [16, lote  1875] pérdida: 0.016\n",
            "Época [17, lote   157] pérdida: 0.177\n",
            "Época [17, lote   313] pérdida: 0.090\n",
            "Época [17, lote   469] pérdida: 0.062\n",
            "Época [17, lote   625] pérdida: 0.045\n",
            "Época [17, lote   782] pérdida: 0.035\n",
            "Época [17, lote   938] pérdida: 0.030\n",
            "Época [17, lote  1094] pérdida: 0.026\n",
            "Época [17, lote  1250] pérdida: 0.024\n",
            "Época [17, lote  1407] pérdida: 0.021\n",
            "Época [17, lote  1563] pérdida: 0.018\n",
            "Época [17, lote  1719] pérdida: 0.017\n",
            "Época [17, lote  1875] pérdida: 0.017\n",
            "Época [18, lote   157] pérdida: 0.174\n",
            "Época [18, lote   313] pérdida: 0.083\n",
            "Época [18, lote   469] pérdida: 0.060\n",
            "Época [18, lote   625] pérdida: 0.041\n",
            "Época [18, lote   782] pérdida: 0.037\n",
            "Época [18, lote   938] pérdida: 0.030\n",
            "Época [18, lote  1094] pérdida: 0.025\n",
            "Época [18, lote  1250] pérdida: 0.023\n",
            "Época [18, lote  1407] pérdida: 0.019\n",
            "Época [18, lote  1563] pérdida: 0.019\n",
            "Época [18, lote  1719] pérdida: 0.017\n",
            "Época [18, lote  1875] pérdida: 0.015\n",
            "Época [19, lote   157] pérdida: 0.153\n",
            "Época [19, lote   313] pérdida: 0.083\n",
            "Época [19, lote   469] pérdida: 0.061\n",
            "Época [19, lote   625] pérdida: 0.047\n",
            "Época [19, lote   782] pérdida: 0.035\n",
            "Época [19, lote   938] pérdida: 0.031\n",
            "Época [19, lote  1094] pérdida: 0.024\n",
            "Época [19, lote  1250] pérdida: 0.021\n",
            "Época [19, lote  1407] pérdida: 0.020\n",
            "Época [19, lote  1563] pérdida: 0.019\n",
            "Época [19, lote  1719] pérdida: 0.017\n",
            "Época [19, lote  1875] pérdida: 0.014\n",
            "Época [20, lote   157] pérdida: 0.155\n",
            "Época [20, lote   313] pérdida: 0.077\n",
            "Época [20, lote   469] pérdida: 0.054\n",
            "Época [20, lote   625] pérdida: 0.042\n",
            "Época [20, lote   782] pérdida: 0.034\n",
            "Época [20, lote   938] pérdida: 0.027\n",
            "Época [20, lote  1094] pérdida: 0.025\n",
            "Época [20, lote  1250] pérdida: 0.022\n",
            "Época [20, lote  1407] pérdida: 0.021\n",
            "Época [20, lote  1563] pérdida: 0.018\n",
            "Época [20, lote  1719] pérdida: 0.018\n",
            "Época [20, lote  1875] pérdida: 0.015\n",
            "Entrenamiento finalizado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardar modelo"
      ],
      "metadata": {
        "id": "dPLWY8usZeAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = './mnist_cnn_net.pth'\n",
        "torch.save(net.state_dict(), ruta)"
      ],
      "metadata": {
        "id": "qXgUoj8UZhxy"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar modelo"
      ],
      "metadata": {
        "id": "60AfTCtrac3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(ruta, weights_only=True))"
      ],
      "metadata": {
        "id": "B-mEMPk_afyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848a96f1-8570-4b84-eb6d-0780863eb270"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de modelo"
      ],
      "metadata": {
        "id": "15nK3-vwmbaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prueba de modelo por cada clase\n",
        "correct_pred = {classname: 0 for classname in clases}\n",
        "total_pred = {classname: 0 for classname in clases}\n",
        "\n",
        "# no se necesita la gradiente ya que se entrena una sola vez\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        imagenes, etiquetas = data\n",
        "        salidas = net(imagenes)\n",
        "        _, predicciones = torch.max(salidas, 1)\n",
        "        # collección de predicciones correctas por cada clase\n",
        "        for etiqueta, prediccion in zip(etiquetas, predicciones):\n",
        "            if etiqueta == prediccion:\n",
        "                correct_pred[clases[etiqueta]] += 1\n",
        "            total_pred[clases[etiqueta]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} es {accuracy:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvSi62zvmfrx",
        "outputId": "77f76d6f-5a60-4c86-8aac-9c669c3f601b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: T-shirt/top es 81.20 %\n",
            "Accuracy for class: Trouser es 97.80 %\n",
            "Accuracy for class: Pullover es 88.00 %\n",
            "Accuracy for class: Dress es 89.40 %\n",
            "Accuracy for class: Coat  es 83.60 %\n",
            "Accuracy for class: Sandal es 96.30 %\n",
            "Accuracy for class: Shirt es 67.80 %\n",
            "Accuracy for class: Sneaker es 97.70 %\n",
            "Accuracy for class: Bag   es 96.90 %\n",
            "Accuracy for class: Ankle boot es 95.20 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precisión total"
      ],
      "metadata": {
        "id": "4aszH61ZoRC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correcto = 0\n",
        "total = 0\n",
        "# no se necesita la gradiente\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        imagenes, etiquetas = data\n",
        "        # calculate outputs by running images through the network\n",
        "        salidas = net(imagenes)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, prediccion = torch.max(salidas, 1)\n",
        "        total += etiquetas.size(0)\n",
        "        correcto += (prediccion == etiquetas).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correcto // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SrcaDOvoUJv",
        "outputId": "bf0f513c-89cc-4c97-a216-64f9bd94667c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "-QaXo1Btwu1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015bb556-859b-4a88-9bd8-eeea30fb79b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n",
            "tensor([[ 3.2571, -9.8655, 16.0035, -0.5538,  7.7646, -7.0548,  9.6464, -7.3217,\n",
            "         -2.8486, -6.8777]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#input = torch.randn(1, 1, 28, 28) # N lotes o batch, canales de entrada, dimensiones (w, h)\n",
        "input = train_features[0].unsqueeze(0) # enviar de uno a uno\n",
        "print(input.size())\n",
        "out = net(input)\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}