{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "noe5nn1swu1a"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DEpz0Nwu1c"
      },
      "source": [
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the `torch.nn` package.\n",
        "\n",
        "Now that you had a glimpse of `autograd`, `nn` depends on `autograd` to\n",
        "define models and differentiate them. An `nn.Module` contains layers,\n",
        "and a method `forward(input)` that returns the `output`.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        "![convnet](https://pytorch.org/tutorials/_static/img/mnist.png)\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "-   Define the neural network that has some learnable parameters (or\n",
        "    weights)\n",
        "-   Iterate over a dataset of inputs\n",
        "-   Process input through the network\n",
        "-   Compute the loss (how far is the output from being correct)\n",
        "-   Propagate gradients back into the network's parameters\n",
        "-   Update the weights of the network, typically using a simple update\n",
        "    rule: `weight = weight - learning_rate * gradient`\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let's define this network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntB3yDL1wu1d",
        "outputId": "d9fcbdff-6fb9-4daa-f15c-361de5c0d7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 4*4 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    # operaciones\n",
        "    def forward(self, input):\n",
        "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a (N, 16, 10, 10) Tensor\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # Fully connected layer F5: (N, 400) Tensor input,\n",
        "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        # Fully connected layer F6: (N, 120) Tensor input,\n",
        "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
        "        # outputs a (N, 10) Tensor\n",
        "        output = self.fc3(f6)\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNiY7T9wu1e"
      },
      "source": [
        "You just have to define the `forward` function, and the `backward`\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using `autograd`. You can use any of the Tensor operations in the\n",
        "`forward` function.\n",
        "\n",
        "The learnable parameters of a model are returned by `net.parameters()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG3ESNOgwu1e",
        "outputId": "a439593c-8429-4e12-93c9-bccce7472e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list(net.parameters()) # filtros con pesos aleatorios"
      ],
      "metadata": {
        "id": "1726reoTymZs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar dataset"
      ],
      "metadata": {
        "id": "i4ucMeoYV5fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "# crear variables para entrenamiento para pruebas\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "print(training_data)\n",
        "print(test_data)\n",
        "clases=training_data.classes\n",
        "print(clases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypVIoh4b2OkE",
        "outputId": "1317b7e8-833b-4142-b5ce-0be566a01d25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 302kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.61MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 7.63MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n",
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n",
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtener caractertisticas y etiquetas de las imágenes"
      ],
      "metadata": {
        "id": "OY1t9v-4Vh0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#obtener caracteristicas de imagenes y etiquetas\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Tamaño de batch, dimensiones y canales: {train_features.size()}\")\n",
        "print(f\"Número de etiquetas por cada batch: {train_labels.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eeIFmXk-3-g",
        "outputId": "40e4f8fa-d17b-4f8c-8209-0f1c2d2a2568"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de batch, dimensiones y canales: torch.Size([32, 1, 28, 28])\n",
            "Número de etiquetas por cada batch: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir función de perdida y optimizador"
      ],
      "metadata": {
        "id": "VfHAnvWfWoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterio = nn.CrossEntropyLoss()\n",
        "# momentum sirve para acelerar el proceso de encontrar el valor más cercano a cero\n",
        "optimizador = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "qBPyCIhzWvmb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurar uso de GPU"
      ],
      "metadata": {
        "id": "3TNLdnv9Ngmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# elección de arquitectura\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Usando:\",device)\n",
        "\n",
        "# Mover el modelo a GPU/CPU\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2T0W9DwNmbF",
        "outputId": "e02bf57a-9462-405f-dd5a-49a82f1a2b93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo de CNN"
      ],
      "metadata": {
        "id": "gf7e25JeWQ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento de modelo\n",
        "mini_batch = 5000\n",
        "for epoca in range(20):  # número de épocas\n",
        "\n",
        "    running_loss = 0.0\n",
        "    umbral = mini_batch\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the entradas; data is a list of [entradas, etiquetas]\n",
        "        entradas, etiquetas = data\n",
        "        # mover datos a GPU/CPU\n",
        "        entradas, etiquetas = entradas.to(device), etiquetas.to(device)\n",
        "        # gradiente de ceros\n",
        "        optimizador.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        salidas = net(entradas) # envío\n",
        "        perdida = criterio(salidas, etiquetas) # calculo diferencia o pérdida\n",
        "        perdida.backward() # Actualiza pesos\n",
        "        optimizador.step() # optimizo\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += perdida.item() # perdida acumulada\n",
        "        if ((i+1) * batch_size) >= umbral:    # print every 5000 mini-batches\n",
        "            print(f'Época [{epoca + 1}, lote {i + 1:5d}] pérdida: {running_loss / (i+1):.3f}')\n",
        "            running_loss = 0.0\n",
        "            umbral += mini_batch\n",
        "\n",
        "print('Entrenamiento finalizado')"
      ],
      "metadata": {
        "id": "LhWRm5--WWzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99705ad-1b34-42ab-f0ff-a0ae274c304b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [1, lote   157] pérdida: 1.694\n",
            "Época [1, lote   313] pérdida: 0.391\n",
            "Época [1, lote   469] pérdida: 0.219\n",
            "Época [1, lote   625] pérdida: 0.144\n",
            "Época [1, lote   782] pérdida: 0.110\n",
            "Época [1, lote   938] pérdida: 0.085\n",
            "Época [1, lote  1094] pérdida: 0.072\n",
            "Época [1, lote  1250] pérdida: 0.060\n",
            "Época [1, lote  1407] pérdida: 0.051\n",
            "Época [1, lote  1563] pérdida: 0.044\n",
            "Época [1, lote  1719] pérdida: 0.040\n",
            "Época [1, lote  1875] pérdida: 0.035\n",
            "Época [2, lote   157] pérdida: 0.415\n",
            "Época [2, lote   313] pérdida: 0.204\n",
            "Época [2, lote   469] pérdida: 0.130\n",
            "Época [2, lote   625] pérdida: 0.099\n",
            "Época [2, lote   782] pérdida: 0.071\n",
            "Época [2, lote   938] pérdida: 0.057\n",
            "Época [2, lote  1094] pérdida: 0.054\n",
            "Época [2, lote  1250] pérdida: 0.045\n",
            "Época [2, lote  1407] pérdida: 0.041\n",
            "Época [2, lote  1563] pérdida: 0.035\n",
            "Época [2, lote  1719] pérdida: 0.032\n",
            "Época [2, lote  1875] pérdida: 0.028\n",
            "Época [3, lote   157] pérdida: 0.329\n",
            "Época [3, lote   313] pérdida: 0.169\n",
            "Época [3, lote   469] pérdida: 0.108\n",
            "Época [3, lote   625] pérdida: 0.075\n",
            "Época [3, lote   782] pérdida: 0.066\n",
            "Época [3, lote   938] pérdida: 0.054\n",
            "Época [3, lote  1094] pérdida: 0.045\n",
            "Época [3, lote  1250] pérdida: 0.043\n",
            "Época [3, lote  1407] pérdida: 0.035\n",
            "Época [3, lote  1563] pérdida: 0.033\n",
            "Época [3, lote  1719] pérdida: 0.027\n",
            "Época [3, lote  1875] pérdida: 0.026\n",
            "Época [4, lote   157] pérdida: 0.297\n",
            "Época [4, lote   313] pérdida: 0.150\n",
            "Época [4, lote   469] pérdida: 0.097\n",
            "Época [4, lote   625] pérdida: 0.077\n",
            "Época [4, lote   782] pérdida: 0.059\n",
            "Época [4, lote   938] pérdida: 0.052\n",
            "Época [4, lote  1094] pérdida: 0.039\n",
            "Época [4, lote  1250] pérdida: 0.038\n",
            "Época [4, lote  1407] pérdida: 0.033\n",
            "Época [4, lote  1563] pérdida: 0.028\n",
            "Época [4, lote  1719] pérdida: 0.026\n",
            "Época [4, lote  1875] pérdida: 0.025\n",
            "Época [5, lote   157] pérdida: 0.261\n",
            "Época [5, lote   313] pérdida: 0.144\n",
            "Época [5, lote   469] pérdida: 0.093\n",
            "Época [5, lote   625] pérdida: 0.071\n",
            "Época [5, lote   782] pérdida: 0.057\n",
            "Época [5, lote   938] pérdida: 0.044\n",
            "Época [5, lote  1094] pérdida: 0.036\n",
            "Época [5, lote  1250] pérdida: 0.037\n",
            "Época [5, lote  1407] pérdida: 0.032\n",
            "Época [5, lote  1563] pérdida: 0.026\n",
            "Época [5, lote  1719] pérdida: 0.025\n",
            "Época [5, lote  1875] pérdida: 0.022\n",
            "Época [6, lote   157] pérdida: 0.252\n",
            "Época [6, lote   313] pérdida: 0.129\n",
            "Época [6, lote   469] pérdida: 0.090\n",
            "Época [6, lote   625] pérdida: 0.067\n",
            "Época [6, lote   782] pérdida: 0.054\n",
            "Época [6, lote   938] pérdida: 0.044\n",
            "Época [6, lote  1094] pérdida: 0.036\n",
            "Época [6, lote  1250] pérdida: 0.034\n",
            "Época [6, lote  1407] pérdida: 0.031\n",
            "Época [6, lote  1563] pérdida: 0.027\n",
            "Época [6, lote  1719] pérdida: 0.024\n",
            "Época [6, lote  1875] pérdida: 0.020\n",
            "Época [7, lote   157] pérdida: 0.242\n",
            "Época [7, lote   313] pérdida: 0.113\n",
            "Época [7, lote   469] pérdida: 0.080\n",
            "Época [7, lote   625] pérdida: 0.063\n",
            "Época [7, lote   782] pérdida: 0.052\n",
            "Época [7, lote   938] pérdida: 0.042\n",
            "Época [7, lote  1094] pérdida: 0.034\n",
            "Época [7, lote  1250] pérdida: 0.033\n",
            "Época [7, lote  1407] pérdida: 0.031\n",
            "Época [7, lote  1563] pérdida: 0.025\n",
            "Época [7, lote  1719] pérdida: 0.023\n",
            "Época [7, lote  1875] pérdida: 0.021\n",
            "Época [8, lote   157] pérdida: 0.226\n",
            "Época [8, lote   313] pérdida: 0.114\n",
            "Época [8, lote   469] pérdida: 0.076\n",
            "Época [8, lote   625] pérdida: 0.062\n",
            "Época [8, lote   782] pérdida: 0.051\n",
            "Época [8, lote   938] pérdida: 0.037\n",
            "Época [8, lote  1094] pérdida: 0.037\n",
            "Época [8, lote  1250] pérdida: 0.029\n",
            "Época [8, lote  1407] pérdida: 0.026\n",
            "Época [8, lote  1563] pérdida: 0.024\n",
            "Época [8, lote  1719] pérdida: 0.024\n",
            "Época [8, lote  1875] pérdida: 0.021\n",
            "Época [9, lote   157] pérdida: 0.225\n",
            "Época [9, lote   313] pérdida: 0.107\n",
            "Época [9, lote   469] pérdida: 0.073\n",
            "Época [9, lote   625] pérdida: 0.061\n",
            "Época [9, lote   782] pérdida: 0.046\n",
            "Época [9, lote   938] pérdida: 0.038\n",
            "Época [9, lote  1094] pérdida: 0.032\n",
            "Época [9, lote  1250] pérdida: 0.029\n",
            "Época [9, lote  1407] pérdida: 0.026\n",
            "Época [9, lote  1563] pérdida: 0.023\n",
            "Época [9, lote  1719] pérdida: 0.022\n",
            "Época [9, lote  1875] pérdida: 0.019\n",
            "Época [10, lote   157] pérdida: 0.229\n",
            "Época [10, lote   313] pérdida: 0.109\n",
            "Época [10, lote   469] pérdida: 0.069\n",
            "Época [10, lote   625] pérdida: 0.054\n",
            "Época [10, lote   782] pérdida: 0.043\n",
            "Época [10, lote   938] pérdida: 0.037\n",
            "Época [10, lote  1094] pérdida: 0.031\n",
            "Época [10, lote  1250] pérdida: 0.030\n",
            "Época [10, lote  1407] pérdida: 0.026\n",
            "Época [10, lote  1563] pérdida: 0.022\n",
            "Época [10, lote  1719] pérdida: 0.021\n",
            "Época [10, lote  1875] pérdida: 0.019\n",
            "Época [11, lote   157] pérdida: 0.208\n",
            "Época [11, lote   313] pérdida: 0.110\n",
            "Época [11, lote   469] pérdida: 0.069\n",
            "Época [11, lote   625] pérdida: 0.055\n",
            "Época [11, lote   782] pérdida: 0.045\n",
            "Época [11, lote   938] pérdida: 0.034\n",
            "Época [11, lote  1094] pérdida: 0.029\n",
            "Época [11, lote  1250] pérdida: 0.029\n",
            "Época [11, lote  1407] pérdida: 0.024\n",
            "Época [11, lote  1563] pérdida: 0.020\n",
            "Época [11, lote  1719] pérdida: 0.020\n",
            "Época [11, lote  1875] pérdida: 0.018\n",
            "Época [12, lote   157] pérdida: 0.195\n",
            "Época [12, lote   313] pérdida: 0.105\n",
            "Época [12, lote   469] pérdida: 0.065\n",
            "Época [12, lote   625] pérdida: 0.048\n",
            "Época [12, lote   782] pérdida: 0.046\n",
            "Época [12, lote   938] pérdida: 0.036\n",
            "Época [12, lote  1094] pérdida: 0.031\n",
            "Época [12, lote  1250] pérdida: 0.027\n",
            "Época [12, lote  1407] pérdida: 0.024\n",
            "Época [12, lote  1563] pérdida: 0.023\n",
            "Época [12, lote  1719] pérdida: 0.019\n",
            "Época [12, lote  1875] pérdida: 0.017\n",
            "Época [13, lote   157] pérdida: 0.180\n",
            "Época [13, lote   313] pérdida: 0.105\n",
            "Época [13, lote   469] pérdida: 0.059\n",
            "Época [13, lote   625] pérdida: 0.049\n",
            "Época [13, lote   782] pérdida: 0.041\n",
            "Época [13, lote   938] pérdida: 0.034\n",
            "Época [13, lote  1094] pérdida: 0.028\n",
            "Época [13, lote  1250] pérdida: 0.024\n",
            "Época [13, lote  1407] pérdida: 0.022\n",
            "Época [13, lote  1563] pérdida: 0.021\n",
            "Época [13, lote  1719] pérdida: 0.018\n",
            "Época [13, lote  1875] pérdida: 0.017\n",
            "Época [14, lote   157] pérdida: 0.176\n",
            "Época [14, lote   313] pérdida: 0.093\n",
            "Época [14, lote   469] pérdida: 0.063\n",
            "Época [14, lote   625] pérdida: 0.051\n",
            "Época [14, lote   782] pérdida: 0.041\n",
            "Época [14, lote   938] pérdida: 0.034\n",
            "Época [14, lote  1094] pérdida: 0.029\n",
            "Época [14, lote  1250] pérdida: 0.024\n",
            "Época [14, lote  1407] pérdida: 0.022\n",
            "Época [14, lote  1563] pérdida: 0.020\n",
            "Época [14, lote  1719] pérdida: 0.019\n",
            "Época [14, lote  1875] pérdida: 0.018\n",
            "Época [15, lote   157] pérdida: 0.177\n",
            "Época [15, lote   313] pérdida: 0.079\n",
            "Época [15, lote   469] pérdida: 0.064\n",
            "Época [15, lote   625] pérdida: 0.047\n",
            "Época [15, lote   782] pérdida: 0.037\n",
            "Época [15, lote   938] pérdida: 0.033\n",
            "Época [15, lote  1094] pérdida: 0.028\n",
            "Época [15, lote  1250] pérdida: 0.023\n",
            "Época [15, lote  1407] pérdida: 0.022\n",
            "Época [15, lote  1563] pérdida: 0.019\n",
            "Época [15, lote  1719] pérdida: 0.019\n",
            "Época [15, lote  1875] pérdida: 0.016\n",
            "Época [16, lote   157] pérdida: 0.168\n",
            "Época [16, lote   313] pérdida: 0.085\n",
            "Época [16, lote   469] pérdida: 0.061\n",
            "Época [16, lote   625] pérdida: 0.047\n",
            "Época [16, lote   782] pérdida: 0.036\n",
            "Época [16, lote   938] pérdida: 0.031\n",
            "Época [16, lote  1094] pérdida: 0.027\n",
            "Época [16, lote  1250] pérdida: 0.024\n",
            "Época [16, lote  1407] pérdida: 0.022\n",
            "Época [16, lote  1563] pérdida: 0.018\n",
            "Época [16, lote  1719] pérdida: 0.018\n",
            "Época [16, lote  1875] pérdida: 0.017\n",
            "Época [17, lote   157] pérdida: 0.171\n",
            "Época [17, lote   313] pérdida: 0.092\n",
            "Época [17, lote   469] pérdida: 0.056\n",
            "Época [17, lote   625] pérdida: 0.045\n",
            "Época [17, lote   782] pérdida: 0.037\n",
            "Época [17, lote   938] pérdida: 0.030\n",
            "Época [17, lote  1094] pérdida: 0.027\n",
            "Época [17, lote  1250] pérdida: 0.024\n",
            "Época [17, lote  1407] pérdida: 0.021\n",
            "Época [17, lote  1563] pérdida: 0.022\n",
            "Época [17, lote  1719] pérdida: 0.018\n",
            "Época [17, lote  1875] pérdida: 0.016\n",
            "Época [18, lote   157] pérdida: 0.161\n",
            "Época [18, lote   313] pérdida: 0.081\n",
            "Época [18, lote   469] pérdida: 0.062\n",
            "Época [18, lote   625] pérdida: 0.044\n",
            "Época [18, lote   782] pérdida: 0.035\n",
            "Época [18, lote   938] pérdida: 0.033\n",
            "Época [18, lote  1094] pérdida: 0.026\n",
            "Época [18, lote  1250] pérdida: 0.023\n",
            "Época [18, lote  1407] pérdida: 0.020\n",
            "Época [18, lote  1563] pérdida: 0.018\n",
            "Época [18, lote  1719] pérdida: 0.016\n",
            "Época [18, lote  1875] pérdida: 0.016\n",
            "Época [19, lote   157] pérdida: 0.168\n",
            "Época [19, lote   313] pérdida: 0.091\n",
            "Época [19, lote   469] pérdida: 0.057\n",
            "Época [19, lote   625] pérdida: 0.042\n",
            "Época [19, lote   782] pérdida: 0.034\n",
            "Época [19, lote   938] pérdida: 0.027\n",
            "Época [19, lote  1094] pérdida: 0.024\n",
            "Época [19, lote  1250] pérdida: 0.020\n",
            "Época [19, lote  1407] pérdida: 0.023\n",
            "Época [19, lote  1563] pérdida: 0.020\n",
            "Época [19, lote  1719] pérdida: 0.016\n",
            "Época [19, lote  1875] pérdida: 0.015\n",
            "Época [20, lote   157] pérdida: 0.148\n",
            "Época [20, lote   313] pérdida: 0.079\n",
            "Época [20, lote   469] pérdida: 0.056\n",
            "Época [20, lote   625] pérdida: 0.041\n",
            "Época [20, lote   782] pérdida: 0.034\n",
            "Época [20, lote   938] pérdida: 0.028\n",
            "Época [20, lote  1094] pérdida: 0.022\n",
            "Época [20, lote  1250] pérdida: 0.022\n",
            "Época [20, lote  1407] pérdida: 0.020\n",
            "Época [20, lote  1563] pérdida: 0.017\n",
            "Época [20, lote  1719] pérdida: 0.016\n",
            "Época [20, lote  1875] pérdida: 0.016\n",
            "Entrenamiento finalizado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validación en entrenamiento"
      ],
      "metadata": {
        "id": "K08JSxx_9VYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correcto = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for entradas, etiquetas in train_dataloader:\n",
        "        entradas, etiquetas = entradas.to(device), etiquetas.to(device)\n",
        "        salidas = net(entradas)\n",
        "        _, pred = torch.max(salidas, 1) # mayor valor\n",
        "        total += etiquetas.size(0)\n",
        "        correcto += (pred == etiquetas).sum().item()\n",
        "\n",
        "print(f'Precisión en entrenamiento: {100 * correcto / total:.2f}%')"
      ],
      "metadata": {
        "id": "-4NHlLLE9Zvk",
        "outputId": "5c4648e4-1995-4db3-efca-b8fe0b391d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión en entrenamiento: 94.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardar modelo"
      ],
      "metadata": {
        "id": "dPLWY8usZeAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = './mnist_cnn_net.pth'\n",
        "torch.save(net.state_dict(), ruta)"
      ],
      "metadata": {
        "id": "qXgUoj8UZhxy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar modelo"
      ],
      "metadata": {
        "id": "60AfTCtrac3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(ruta, weights_only=True))"
      ],
      "metadata": {
        "id": "B-mEMPk_afyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d232821-798a-42db-ce0a-22a23cca7782"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de modelo"
      ],
      "metadata": {
        "id": "15nK3-vwmbaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prueba de modelo por cada clase\n",
        "correct_pred = {classname: 0 for classname in clases}\n",
        "total_pred = {classname: 0 for classname in clases}\n",
        "\n",
        "# no se necesita la gradiente ya que se entrena una sola vez\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        imagenes, etiquetas = data\n",
        "        salidas = net(imagenes)\n",
        "        _, predicciones = torch.max(salidas, 1)\n",
        "        # collección de predicciones correctas por cada clase\n",
        "        for etiqueta, prediccion in zip(etiquetas, predicciones):\n",
        "            if etiqueta == prediccion:\n",
        "                correct_pred[clases[etiqueta]] += 1\n",
        "            total_pred[clases[etiqueta]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} es {accuracy:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvSi62zvmfrx",
        "outputId": "3380dd55-d139-4a65-cf19-ba15565fb8f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: T-shirt/top es 83.30 %\n",
            "Accuracy for class: Trouser es 96.90 %\n",
            "Accuracy for class: Pullover es 86.50 %\n",
            "Accuracy for class: Dress es 91.80 %\n",
            "Accuracy for class: Coat  es 84.20 %\n",
            "Accuracy for class: Sandal es 98.10 %\n",
            "Accuracy for class: Shirt es 65.90 %\n",
            "Accuracy for class: Sneaker es 96.50 %\n",
            "Accuracy for class: Bag   es 96.30 %\n",
            "Accuracy for class: Ankle boot es 96.40 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precisión total"
      ],
      "metadata": {
        "id": "4aszH61ZoRC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correcto = 0\n",
        "total = 0\n",
        "# no se necesita la gradiente\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        imagenes, etiquetas = data\n",
        "        # calculate outputs by running images through the network\n",
        "        salidas = net(imagenes)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, prediccion = torch.max(salidas, 1)\n",
        "        total += etiquetas.size(0)\n",
        "        correcto += (prediccion == etiquetas).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correcto / total:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SrcaDOvoUJv",
        "outputId": "1308545b-fe83-4b95-ca51-823a9c50846f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89.59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-QaXo1Btwu1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e2d7d0-f332-4086-e3d0-c93915b96c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n",
            "tensor([[ 16.5396, -10.5250,   8.8695,   0.6373,  -0.2618,  -7.2057,   9.7469,\n",
            "          -8.4352,  -1.0282,  -8.5567]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#input = torch.randn(1, 1, 28, 28) # N lotes o batch, canales de entrada, dimensiones (w, h)\n",
        "input = train_features[0].unsqueeze(0) # enviar de uno a uno\n",
        "print(input.size())\n",
        "out = net(input)\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}